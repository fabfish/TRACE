# chat_interactive.py

import argparse
import logging
import torch
from transformers import pipeline, set_seed, AutoTokenizer, AutoModelForCausalLM

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="ä¸NPUä¸Šçš„è¯­è¨€æ¨¡å‹è¿›è¡Œäº¤äº’å¼èŠå¤©ã€‚")
    parser.add_argument(
        "--path", 
        type=str, 
        required=True, 
        help="åŒ…å«å·²è®­ç»ƒæ¨¡å‹å’Œtokenizerçš„ç›®å½•è·¯å¾„ã€‚"
    )
    parser.add_argument(
        "--max_new_tokens", 
        type=int, 
        default=256, 
        help="æ¯æ¬¡å›å¤ç”Ÿæˆçš„æœ€å¤§æ–°è¯å…ƒ(token)æ•°é‡ã€‚"
    )
    return parser.parse_args()

def setup_pipeline(path, device):
    """è®¾ç½®å¹¶è¿”å›ä¸€ä¸ªtransformers pipeline"""
    print(f"ğŸš€ æ­£åœ¨ä» '{path}' åŠ è½½æ¨¡å‹å’Œtokenizer...")
    try:
        # ä½¿ç”¨ AutoClass è‡ªåŠ¨è¯†åˆ«å¹¶åŠ è½½å¯¹åº”çš„æ¨¡å‹å’Œtokenizerï¼Œæ›´å…·é€šç”¨æ€§
        tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            path,
            torch_dtype=torch.bfloat16,  # åœ¨NPUå’Œç°ä»£GPUä¸Šï¼Œbfloat16æ€§èƒ½å’Œç¨³å®šæ€§æ›´ä½³
            trust_remote_code=True
        )
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        print("ğŸ‘‰ è¯·ç¡®ä¿æä¾›çš„è·¯å¾„æ­£ç¡®ï¼Œå¹¶ä¸”ç›®å½•ä¸­åŒ…å«æ‰€æœ‰å¿…è¦çš„æ–‡ä»¶ï¼ˆå¦‚config.json, pytorch_model.binç­‰ï¼‰ã€‚")
        exit()

    # å¦‚æœtokenizeræ²¡æœ‰pad_tokenï¼Œé€šå¸¸å¯ä»¥å°†å…¶è®¾ç½®ä¸ºeos_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = model.config.eos_token_id

    print(f"ğŸ§  æ¨¡å‹å·²åŠ è½½ï¼Œæ­£åœ¨éƒ¨ç½²åˆ°è®¾å¤‡: {device}...")
    
    # åˆ›å»ºpipelineï¼Œè¿™æ˜¯ä¸æ¨¡å‹äº¤äº’çš„æ¨èæ–¹å¼
    generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=device,
        # å…³é”®å‚æ•°ï¼šè®¾ç½®åï¼Œpipelineå°†åªè¿”å›æ–°ç”Ÿæˆçš„æ–‡æœ¬ï¼Œæ— éœ€æˆ‘ä»¬æ‰‹åŠ¨å¤„ç†
        return_full_text=False
    )
    print("âœ… Pipelineå‡†å¤‡å°±ç»ªï¼")
    return generator

def main(args):
    # è‡ªåŠ¨æ£€æµ‹NPUï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨CPU
    if torch.npu.is_available():
        device = "npu:0"
    else:
        device = "cpu"
        print("âš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°NPUï¼Œå°†ä½¿ç”¨CPUè¿è¡Œï¼Œé€Ÿåº¦å¯èƒ½è¾ƒæ…¢ã€‚")
    
    generator = setup_pipeline(args.path, device)
    set_seed(42)

    # ä½¿ç”¨ä¸€ä¸ªåˆ—è¡¨æ¥ä¼˜é›…åœ°ç»´æŠ¤å¯¹è¯å†å²
    history = []
    
    print("\n" + "="*30)
    print("      æ¬¢è¿æ¥åˆ°NPUæ¨¡å‹èŠå¤©å®¤      ")
    print("="*30)
    print("ğŸ‘‰ è¾“å…¥ 'quit' é€€å‡º, è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²ã€‚")

    while True:
        user_prompt = input("You: ")
        
        if user_prompt.lower() in ["quit", "exit"]:
            print("ğŸ‘‹ å†è§ï¼")
            break
        if user_prompt.lower() == "clear":
            history = []
            print("\nâœ¨ å¯¹è¯å†å²å·²æ¸…ç©º âœ¨\n")
            continue

        # 1. æ ¼å¼åŒ–åŒ…å«å†å²è®°å½•çš„å®Œæ•´æç¤º
        # æ¨¡æ¿: "Human: ...\nAssistant: ...\nHuman: ...\nAssistant:"
        full_prompt = ""
        for h, a in history:
            full_prompt += f"Human: {h}\nAssistant: {a}\n"
        full_prompt += f"Human: {user_prompt}\nAssistant:"
        
        # 2. æ¨¡å‹ç”Ÿæˆå›å¤
        print("Bot: ...âœï¸")
        response = generator(full_prompt, max_new_tokens=args.max_new_tokens)
        
        # å› ä¸ºè®¾ç½®äº† return_full_text=Falseï¼Œè¿™é‡Œç›´æ¥å°±æ˜¯å¹²å‡€çš„æ–°å›å¤
        model_output = response[0]['generated_text'].strip()
        
        # 3. æ‰“å°å›å¤å¹¶æ›´æ–°å†å²è®°å½•
        # ä½¿ç”¨ \r å’Œ end="" å¯ä»¥å°† "...âœï¸" è¦†ç›–æ‰ï¼Œæ˜¾å¾—æ›´æµç•…
        print(f"\rBot: {model_output}", end="\n\n") 
        history.append((user_prompt, model_output))

if __name__ == "__main__":
    # å±è”½transformersçš„ä¸€äº›ä¸å¿…è¦çš„æ—¥å¿—è¾“å‡º
    logging.getLogger("transformers").setLevel(logging.ERROR)
    args = parse_args()
    main(args)